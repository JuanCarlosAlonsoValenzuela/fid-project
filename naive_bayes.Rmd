# Naive Bayes
En este notebook se aplica el modelo probabilístico Naive Bayes
Ventajas e inconvenientes de Naive Bayes
Se ha consultado la documentación del paquete Caret de NB (https://rpubs.com/maulikpatel/224581), partiendo los conocimientos adquiridos con los notebooks estudiados durante el curso.

## Carga de datos
Comenzamos cargando las librerías necesarias.
El paquete klarR es...

```{r}
library(naivebayes)
library("klaR")
library("caret")
library(e1071)
```

Cargamos los datos de entrenamiento y de pruebas preprocesados (Ver notebook data_analysis.Rmd)
```{r}
train <- read.csv("dataset/train_preprocessed.data", sep=",", row.names=NULL, header=TRUE)
head(train)
dim(train)

test <- read.csv("dataset/test_preprocessed.data", sep=",", row.names=NULL, header=TRUE)
head(test)
dim(test)
```

```{r}
summary(train)
```

Los atributos que se usarán para el entrenamiento son:
age, workclass, fnlwgt, education, marital_status, occupation, relationship, race, 
sex, native_country, hours_per_week_categorical, has_capital_gain, has_capital_loss

TODO: CONVERTIR fnlwgt A CATEGÓRICO
Convertimos el atributo age a categórico (grupos de 10 años)

Tener en cuenta que: age es numérico (convertir a categórico?), education_num es redundante, 
capital_gain y capital_loss han sido convertidos a categóricos, hours_per_week es categórico

La variable a predecir es target (Posibles valores: <=50K, >50K).

```{r}
convert_age_to_categorical <- function(x) {
    age <- as.numeric(x['age'])
    decade <- floor(age/10)*10
    decade <- as.character(paste(decade, decade+10, sep="_"))
    return(decade)
}	
```	

```{r}	
decade_train <- apply(train, 1, convert_age_to_categorical)
train <- cbind(train, decade=decade_train)

decade_test <- apply(test, 1, convert_age_to_categorical)
test <- cbind(test, decade=decade_test)
head(train)
head(test)
```

```{r}
transform_data <- function(dataframe) {
    dataframe <- transform(dataframe, age=as.integer(age), workclass=as.factor(workclass),
        fnlwgt=as.numeric(fnlwgt), education=as.factor(education), education_num=as.integer(education_num),
        marital_status=as.factor(marital_status), occupation=as.factor(occupation), relationship=as.factor(relationship),
        race=as.factor(race), sex=as.factor(sex), capital_gain=as.numeric(capital_gain), capital_loss=as.numeric(capital_loss),
        hours_per_week=as.integer(hours_per_week), native_country=as.factor(native_country), target=as.factor(target),
        hours_per_week_categorical=as.factor(hours_per_week_categorical),has_capital_gain=as.factor(has_capital_gain),
        has_capital_loss=as.factor(has_capital_loss), decade=as.factor(decade)
    )

    return(dataframe)
}
```

```{r}
train <- transform_data(train)
summary(train)	
dim(train)

test <- transform_data(test)
summary(test)
dim(test)
```

```{r}
# Removed fnlwgt and age
selected_attributes <- c("decade", "workclass", "education", "marital_status", "occupation", "relationship", 
"race", "sex", "native_country", "hours_per_week_categorical", "has_capital_gain", "has_capital_loss", "target")
train <- train[selected_attributes]

head(train)	
```	

```{r}
test <- test[selected_attributes]

head(test)
```	


```{r}
summary(train)	
summary(test)
```


# Entrenamiento del modelo

```{r}
# help(confusionMatrix)
```
	
```{r}
train_and_evaluate_naive_bayes_with_configuration <- function(conf_number, train_set, test_set, 
                                laplace_value, eps_value, usepoisson_value) {
    # Print configuration used
    # Configuración #X, laplace=Y, eps=Z, usepoisson=W
    print("####################################")
    print(paste("Configuración #", conf_number, ", laplace=", laplace_value, ", eps=", eps_value, ", usepoisson=", usepoisson_value))

    # Train model
    model_nb <- naiveBayes(target ~ ., data=train_set, laplace=laplace_value, eps=eps_value, usepoisson=usepoisson_value)

    # Predict on train set
    y_pred_train <- predict(model_nb, newdata = train_set)

    train_precision <- precision(data=y_pred_train, reference=train_set$target, relevant=">50K")
    train_recall <- recall(data=y_pred_train, reference=train_set$target, relevant=">50K")
    train_f1 <- F_meas(data=y_pred_train, reference=train_set$target, relevant=">50K")

    # Predict on test set
    y_pred_test <- predict(model_nb, newdata = test_set)

    test_precision <- precision(data=y_pred_test, reference=test_set$target, relevant=">50K")
    test_recall <- recall(data=y_pred_test, reference=test_set$target, relevant=">50K")
    test_f1 <- F_meas(data=y_pred_test, reference=test_set$target, relevant=">50K")

    # Print metrics in both training and test set (in a single line)
    print(paste("Train Precision: ", train_precision, " Recall: ", train_recall, " F1-Score ", train_f1))
    print(paste("Test Precision: ", test_precision, " Recall: ", test_recall, " F1-Score ", test_f1))

    # Return F1-Score
    return(list(test_f1, model_nb))
}
```

Definimos los hiperparámetros (explicar que la función Naive Bayes no admite GridSearch, por lo que la hacemos a mano)


```{r}
set.seed(123)

# Se entrenarán un total de 32 modelos, seleccionando el mejor de todos
laplace_values <- c(0, 0.5, 1, 10)
eps_values <- c(0.01, 0.1, 0.3, 1.0)
usepoisson_values <- c(FALSE, TRUE)

# Contador de configuraciones
i<-1

# Orden: Laplace, epsilon y usepoisson
best_configuration <- c(1000, 1000, FALSE)
best_f1_score <- 0.0
best_model <- NULL


for(laplace_value in laplace_values) {
    for(eps_value in eps_values) {
        for(usepoisson_value in usepoisson_values) {

            # Entrenamos el modelo con la configuración actual
            training_result <- train_and_evaluate_naive_bayes_with_configuration(i, train, test, laplace_value, eps_value, usepoisson_value)
            i <- i + 1

            # Commparamos con el mejor modelo actual
            if(training_result[[1]] > best_f1_score) {
                best_f1_score = training_result[[1]]
                best_model = training_result[[2]]
                best_configuration = c(laplace_value, eps_value, usepoisson_value)
            }

        }
    }
}



```

Explicar que configurar los hiperparámetros (suavizado de laplace) no ha tenido ninguna influencia relevante en el resultado final. (Lección aprendida)
Todos los modelos tienen el mismo f1_score

```{r}
# Imprimimos la configuración que obtuvo el mejor modelo
print("/////////////////////////////////////////")
print("Mejor configuración:")
print(paste("Laplace: ", best_configuration[1], ", eps: ", best_configuration[2], ", usepoisson: ", best_configuration[3]))
print("Métricas: ")
print(paste("Precision: ", )

y_pred_train <- predict(best_model, newdata = train)
cm_train <- confusionMatrix(data=y_pred_train, reference=train$target, positive=">50K", mode = "prec_recall")
train_precision <- precision(data=y_pred_train, reference=train$target, relevant=">50K")
train_recall <- recall(data=y_pred_train, reference=train$target, relevant=">50K")
train_f1 <- F_meas(data=y_pred_train, reference=train$target, relevant=">50K")

y_pred_test <- predict(best_model, newdata = test)
cm_test <- confusionMatrix(data=y_pred_test, reference=test$target, positive=">50K", mode = "prec_recall")
test_precision <- precision(data=y_pred_test, reference=test$target, relevant=">50K")
test_recall <- recall(data=y_pred_test, reference=test$target, relevant=">50K")
test_f1 <- F_meas(data=y_pred_test, reference=test$target, relevant=">50K")

print(paste("Precision train: ", train_precision, "Precision test: ", test_precision))
print(paste("Recall train: ", train_recall, "Recall test: ", test_recall))
print(paste("F1-Score train: ", train_f1, " F1-Score test: ", test_f1))

```

Imprimir y explicar matrices de confusión
```{r}
cm_train
cm_test	
```
# Random Forest
En este notebook se ha realizado una clasificación del conjunto de entrenamiento utilizando Random Forest, para ello se ha utilizado la función [randomForest](https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest), así como el paquete [Caret](https://topepo.github.io/caret/) para calcular las métricas

# Carga de datos
Se realiza la carga de las librerías necesarias.
```{r}
set.seed(123)
#install.packages("randomForest")
library(randomForest)
#install.packages("caTools")
library(caTools)
#install.packages("caret")
library("caret")
```

Cargamos los datos de entrenamiento y de pruebas preprocesados (Ver notebook data_analysis.Rmd)

```{r}
train <- read.csv("../dataset/train_preprocessed.data", sep=",", row.names=NULL, header=TRUE)
head(train)
dim(train)

test <- read.csv("../dataset/test_preprocessed.data", sep=",", row.names=NULL, header=TRUE)
head(test)
dim(test)
```

```{r}
summary(train)
dim(train)	
```

# Preparación y preprocesado de los datos 
La clase a predecir es target (Posibles valores: <=50K, >50K).

Definimos una función que transforme los valores de los atributos para que R pueda interpretarlos correctamente, por ejemplo, identificando las variables categóricas como tal, en lugar de como simples strings.

```{r}
transform_data <- function(dataframe) {
    dataframe <- transform(dataframe, age=as.integer(age), workclass=as.factor(workclass),
        fnlwgt=as.numeric(fnlwgt), education=as.factor(education), education_num=as.integer(education_num),
        marital_status=as.factor(marital_status), occupation=as.factor(occupation), relationship=as.factor(relationship),
        race=as.factor(race), sex=as.factor(sex), capital_gain=as.numeric(capital_gain), capital_loss=as.numeric(capital_loss),
        hours_per_week=as.integer(hours_per_week), native_country=as.factor(native_country), target=as.factor(target),
        hours_per_week_categorical=as.factor(hours_per_week_categorical),has_capital_gain=as.factor(has_capital_gain),
        has_capital_loss=as.factor(has_capital_loss)
    )

    return(dataframe)
}
```

Aplicamos la transformación a los conjuntos de entrenamiento y pruebas:

```{r}
train <- transform_data(train)
summary(train)	
dim(train)

test <- transform_data(test)
summary(test)
dim(test)
```

Seleccionamos los atributos específicos que serán usados para entrenar el modelo
```{r}

selected_attributes <- c("age", "fnlwgt", "workclass", "education", "marital_status", "occupation", "relationship", 
"race", "sex", "native_country", "hours_per_week_categorical", "has_capital_gain", "has_capital_loss", "target")

train <- train[selected_attributes]

test <- test[selected_attributes]

levels(test$native_country) <- levels(train$native_country)

```

# Entrenamiento del modelo y optimización de hiperparámetros
Definimos una función que entrene el modelo con una configuración determinada, devolviendo el modelo entrenado y el rendimiento en F1-Score. 
```{r}
train_and_evaluate_random_forest_with_configuration <- function(conf_number, train_set, test_set, 
                                        ntree_value, mtry_value, importance_value) {
    # Imprimir configuración usada
    # Configuración #X, ntree=Y, mtry=Z, importance=W
    print("####################################")
    print(paste("Configuración #", conf_number, ", ntree=", ntree_value, ", mtry=", mtry_value, ", importance=", importance_value))

    # Entrenar modelo
    model_rf <- randomForest(target ~ ., data=train_set,
        ntree=ntree_value, mtry=mtry_value, importance=importance_value)

    # Predecir en el conjunto de entrenamiento
    y_pred_train <- predict(model_rf, new_data = train_set)
    train_precision <- precision(data=y_pred_train, reference=train_set$target, relevant=">50K")
    train_recall <- recall(data=y_pred_train, reference=train_set$target, relevant=">50K")
    train_f1 <- F_meas(data=y_pred_train, reference=train_set$target, relevant=">50K")

    levels(test_set$native_country) <- levels(train_set$native_country)

    # Predecir en el conjunto de pruebas
    y_pred_test <- predict(model_rf, test_set)
    test_precision <- precision(data=y_pred_test, reference=test_set$target, relevant=">50K")
    test_recall <- recall(data=y_pred_test, reference=test_set$target, relevant=">50K")
    test_f1 <- F_meas(data=y_pred_test, reference=test_set$target, relevant=">50K")

    # Imprimir métricas
    print(paste("Train precision: ", train_precision, ", recall: ", train_recall, ", F1-Score: ", train_f1))
    print(paste("Test precision: ", test_precision, ", recall: ", test_recall, ", F1-Score: ", test_f1))

    # Devolver modelo y  f1 score
    return(list(test_f1, model_rf))

}
```

Definimos los valores de los hiperparámetros que se emplearán en grid search.

```{r}
# Hiperparámetros a configurar
ntree_values <- c(10, 100, 500, 1000)
mtry_values <- c(2.0,3.0,4.0,6.0)
importance_values <- c(FALSE, TRUE)

# Contador de configuraciones 
i <- 1

# Orden: ntree, mtry, importance
best_configuration <- list(10, 2.0, FALSE)
best_f1_score <- 0.0
best_model <- NULL

for(ntree_value in ntree_values) {
    for(mtry_value in mtry_values) {
        for(importance_value in importance_values) {
            training_result <- train_and_evaluate_random_forest_with_configuration(i, train, test, ntree_value, mtry_value, importance_value)
            i <- i + 1

            # Comparamos con el mejor modelo actual
            if(training_result[[1]] > best_f1_score) {
                best_f1_score <- training_result[[1]]
                best_configuration <- list(ntree_value, mtry_value, importance_value)
                best_model <- training_result[[2]]
            }
        }
    }
}


```

# Interpretación de los resultados.
Podemos observar que el modelo con mejor rendimiento es el que ha usado la siguiente configuración:
|     n_tree    	|     mtry    	|     importance    	|
|---------------	|-------------	|-------------------	|
|     1000      	|     3       	|     TRUE          	|

Al igual que otros modelos, Random Forest consigue un mejor rendimiento en términos de Recall que de precision. No obstante, en este caso la diferencia no es tan significativa como en otros modelos.
```{r}
print("Mejor configuración:")
print(paste("ntree=", best_configuration[[1]], ", mtry=", best_configuration[[2]], ", importance=", best_configuration[[3]]))
print("Métricas: ")

y_pred_train <- predict(best_model, train)
cm_train <- confusionMatrix(y_pred_train, train$target, positive=">50K", mode= "prec_recall")
train_precision <- precision(data=y_pred_train, reference=train$target, relevant=">50K")
train_recall <- recall(data=y_pred_train, reference=train$target, relevant=">50K")
train_f1 <- F_meas(data=y_pred_train, reference=train$target, relevant=">50K")

y_pred_test <- predict(best_model, test)
cm_test <- confusionMatrix(y_pred_test, test$target, positive=">50K", mode= "prec_recall")
test_precision <- precision(data=y_pred_test, reference=test$target, relevant=">50K")
test_recall <- recall(data=y_pred_test, reference=test$target, relevant=">50K")
test_f1 <- F_meas(data=y_pred_test, reference=test$target, relevant=">50K")

print(paste("Precision train: ", train_precision, "Precision test: ", test_precision))
print(paste("Recall train: ", train_recall, "Recall test: ", test_recall))
print(paste("F1-Score train: ", train_f1, " F1-Score test: ", test_f1))

```


```{r}
cm_train
cm_test	
```

```{r}
```

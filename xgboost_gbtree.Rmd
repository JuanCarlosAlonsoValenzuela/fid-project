# XGBoost tree
En este notebook se aplica XGBoost con el booster basado en árboles. La documentación de esta librería puede encontrarse [aquí](https://xgboost.readthedocs.io/en/stable/R-package/xgboostPresentation.html).

# Carga de datos
Comenzamos, como siempre, importando las librerías necesarias

```{r}
set.seed(123)

install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
# install.packages("xgboost")
library(xgboost)
# install.packages("tidyverse")
library(tidyverse)
# install.packages("drat")
library(drat)
library("ggplot2")
# install.packages("ggrepel")
library(ggrepel)
# install.packages("gridExtra")
library(gridExtra)
library(caret)
```


Cargamos los datos de entrenamiento y de pruebas preprocesados (Ver notebook data_analysis.Rmd)

```{r}
train <- read.csv("dataset/train_preprocessed.data", sep=",", row.names=NULL, header=TRUE)
head(train)
dim(train)

test <- read.csv("dataset/test_preprocessed.data", sep=",", row.names=NULL, header=TRUE)
head(test)
dim(test)
```

# Preparación y preprocesado de los datos 
Convertimos el atributo country en booleano, para que XGBoost pueda trabajar con dicho atributo sin crear un número excesivo de atributos (ya que, de no aplicar este paso, se generaría un atributo para cada uno de los 41 países del dataset)

```{r}
convert_country_to_categorical <- function(x) {
    country <- x['native_country']
    if (country == "United-States") {
        return("usa")
    } else {
        return("not_usa")
    }
}	
```	

Aplicamos la función definida en el chunk anterior al conjunto de entrenamiento y pruebas

```{r}
born_in_usa_train <- apply(train, 1, convert_country_to_categorical)
train <- cbind(train, born_in_usa=born_in_usa_train)
head(train)	
```	

```{r}
born_in_usa_test <- apply(test, 1, convert_country_to_categorical)
test <- cbind(test, born_in_usa=born_in_usa_test)
head(test)
```	

Definimos una función que transforme los valores de los atributos para que R pueda interpretarlos correctamente, por ejemplo, identificando las variables categóricas como tal, en lugar de como simples strings.

```{r}
transform_data <- function(dataframe) {
    dataframe <- transform(dataframe, age=as.integer(age), workclass=as.factor(workclass),
        fnlwgt=as.numeric(fnlwgt), education=as.factor(education), education_num=as.integer(education_num),
        marital_status=as.factor(marital_status), occupation=as.factor(occupation), relationship=as.factor(relationship),
        race=as.factor(race), sex=as.factor(sex), capital_gain=as.numeric(capital_gain), capital_loss=as.numeric(capital_loss),
        hours_per_week=as.integer(hours_per_week), native_country=as.factor(native_country), target=as.factor(target),
        hours_per_week_categorical=as.factor(hours_per_week_categorical),has_capital_gain=as.factor(has_capital_gain),
        has_capital_loss=as.factor(has_capital_loss), born_in_usa=as.factor(born_in_usa)
    )

    return(dataframe)
}
```

```{r}
train <- transform_data(train)
summary(train)
test <- transform_data(test)
summary(test)
```

Seleccionamos las columnas que usaremos para entrenar el modelo

```{r}
selected_attributes <- c("age", "workclass", "fnlwgt", "education", "marital_status",  "occupation", "relationship", 
"race", "sex", "hours_per_week_categorical", "has_capital_gain", "has_capital_loss", "born_in_usa")

x_train <- train[selected_attributes]
y_train <- train$target

x_test <- test[selected_attributes]
y_test <- test$target
``` 

xgboost recibe una matriz, no un dataframe, es necesario convertir las variables categóricas en numéricas, para ello aplicamos one-hot encoding en las columnas categóricas.

```{r}
head(x_train)
```

```{r}
convert_to_one_hot <- function(dataframe) {
    # workclass
    dataframe <- cbind(dataframe, model.matrix(~workclass-1, dataframe))
    dataframe$workclass <- NULL

    # education
    dataframe <- cbind(dataframe, model.matrix(~education-1, dataframe))
    dataframe$education <- NULL

    # marital_status
    dataframe <- cbind(dataframe, model.matrix(~marital_status-1, dataframe))
    dataframe$marital_status <- NULL

    # occupation
    dataframe <- cbind(dataframe, model.matrix(~occupation-1, dataframe))
    dataframe$occupation <- NULL

    # relationship
    dataframe <- cbind(dataframe, model.matrix(~relationship-1, dataframe))
    dataframe$relationship <- NULL
    
    # race
    dataframe <- cbind(dataframe, model.matrix(~race-1, dataframe))
    dataframe$race <- NULL

    # sex
    dataframe <- cbind(dataframe, model.matrix(~sex-1, dataframe))
    dataframe$sex <- NULL

    # hours_per_week_categorical
    dataframe <- cbind(dataframe, model.matrix(~hours_per_week_categorical-1, dataframe))
    dataframe$hours_per_week_categorical <- NULL

    # has_capital_gain
    dataframe <- cbind(dataframe, model.matrix(~has_capital_gain-1, dataframe))
    dataframe$has_capital_gain <- NULL

    # has_capital_loss
    dataframe <- cbind(dataframe, model.matrix(~has_capital_loss-1, dataframe))
    dataframe$has_capital_loss <- NULL

    # born_in_usa
    dataframe <- cbind(dataframe, model.matrix(~born_in_usa-1, dataframe))
    dataframe$born_in_usa <- NULL

    return(dataframe)
}

```

Aplicamos la conversión a one_hot

```{r}
x_train <- convert_to_one_hot(x_train)
head(x_train)

x_test <- convert_to_one_hot(x_test)
head(x_test)
```


```{r}
summary(y_test)
```

```{r}
head(y_test)
y_train <- as.integer(y_train[]==">50K")
y_test <- as.integer(y_test[]==">50K")

head(y_test)
```

# Entrenamiento del modelo y optimización de hiperparámetros

```{r}
x_train <- as.matrix(x_train)
x_test  <- as.matrix(x_test)

class(x_train)[1]
class(y_train)[1]

dtrain <- xgb.DMatrix(x_train, label=y_train)
dtest <- xgb.DMatrix(x_test, label=y_test)	
```	

Definimos los posibles valores que tendrán los hiperparámetros que serán optimizados con grid search.

```{r}

search_grid <- expand.grid(
    gamma = c(0.0, 0.05),
    max_depth = c(10, 100, 150),
    scale_pos_weight = c(0.5, 1.0, 1.5),
    subsample = c(0.5,0.6)
)

```

En las siguientes líneas se aplica la optimización de hiperparámetros, entrenando un total de 36 configuraciones diferentes. Sin embargo, debido a su amplio tiempo de ejecución, estas líneas se han comentado, pueden verse los resultados obtenidos en el csv generado.

```{r}
watchlist <- list(train=dtrain, test=dtest)


# system.time(
#     logloss_error_hyperparameters <- apply(search_grid, 1, function(parameter_list) {

#         # Obtener los parámetros de la configuración
#         current_gamma <- parameter_list[["gamma"]]
#         current_max_depth <- parameter_list[["max_depth"]]
#         current_scale_pos_weight <- parameter_list[["scale_pos_weight"]]
#         current_subsample <- parameter_list[["subsample"]]

#         # Crear el modelo
#         model_gbtree <- xgb.cv(
#             data=dtrain, 
#             gamma = current_gamma,
#             max.depth = current_max_depth,
#             scale_pos_weight = current_scale_pos_weight, 
#             subsample = current_subsample, 
            
#             nrounds = 20,
#             nfold = 5,
#             verbose  = 2,
#             print_every_n = 1,
#             nthread = 2, 
#             eval_metric = "logloss",
#             watchlist = watchlist,
#             objective = "binary:logistic",
#             booster = "gbtree"
#         )

#         validation_scores <- as.data.frame(model_gbtree$evaluation_log)
#         print(colnames(validation_scores))

#         logloss <- tail(validation_scores$test_logloss_mean, 1)
#         t_logloss <- tail(validation_scores$train_logloss_mean, 1)

#         output <- return(c(logloss, t_logloss, current_gamma, current_max_depth, current_scale_pos_weight, current_subsample))

#     })
# )


```


```{r}
# output <- as.data.frame(t(logloss_error_hyperparameters))
# varnames <- c("Test_logloss", "Train_logloss", "gamma", "max_depth", "scale_pos_weight", "subsample")
# names(output) <- varnames
# head(output)
```

```{r}
# write.csv(output, "output_xgboost_tree_booster.csv")	
```

Volvemos a entrenar el mejor modelo con la configuración obtenida
```{r}

best_model <- xgb.train(data=dtrain, 
                        gamma=0.0, 
                        max.depth=10, 
                        scale_pos_weight=1.0, 
                        subsample=0.6, 
                        nrounds=20, 
                        nfold=5, 
                        verbose=2, 
                        print_every_n=1, 
                        nthread=2, 
                        eval_metric="logloss", 
                        watchlist= watchlist, 
                        objective="binary:logistic", 
                        booster="gbtree"
)

```

Imprimimos el rendimiento del modelo  en los conjuntos de entrenamiento y pruebas, así como la matriz de confusión

```{r}

y_pred_train <- predict(best_model, newdata=dtrain)
y_pred_train <- as.integer(y_pred_train>=0.5)
y_pred_train <- as.factor(y_pred_train)

y_train <- as.factor(y_train)

cm_train <- confusionMatrix(data=y_pred_train, reference=y_train, positive="1", mode = "prec_recall")
cm_train
```


```{r}

y_pred_test <- predict(best_model, newdata=dtest)
y_pred_test <- as.integer(y_pred_test>=0.5)
y_pred_test <- as.factor(y_pred_test)

y_test <- as.factor(y_test)

cm_test <- confusionMatrix(data=y_pred_test, reference=y_test, positive="1", mode = "prec_recall")
cm_test
```

```{r}
```
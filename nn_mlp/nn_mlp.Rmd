---
title: "Red neuronal de tipo perceptrón multicapa"
output: html_notebook
---

```{r}
set.seed(123) # se establece la semilla para que cada vez que se ejecute el notebook desde el principio salga lo mismo

require(caret)
library(doParallel)
```

```{r}

train_data <- read.csv("../dataset/train_preprocessed.data", sep=",")
test_data <- read.csv("../dataset/test_preprocessed.data", sep=",")

transform_data <- function(dataframe) {
    dataframe <- transform(dataframe, age=as.integer(age), workclass=as.factor(workclass),
        fnlwgt=as.numeric(fnlwgt), education=as.factor(education), education_num=as.integer(education_num),
        marital_status=as.factor(marital_status), occupation=as.factor(occupation), relationship=as.factor(relationship),
        race=as.factor(race), sex=as.factor(sex), capital_gain=as.numeric(capital_gain), capital_loss=as.numeric(capital_loss),
        hours_per_week=as.integer(hours_per_week), native_country=as.factor(native_country), target=as.factor(target),
        hours_per_week_categorical=as.factor(hours_per_week_categorical),has_capital_gain=as.factor(has_capital_gain),
        has_capital_loss=as.factor(has_capital_loss)
    )

    return(dataframe)
}

train_data <- transform_data(train_data)
test_data <- transform_data(test_data)

```

```{r}
selected_attributes <- c("age", "workclass", "education", "marital_status", "occupation", "relationship", 
                         "race", "sex", "native_country", "hours_per_week_categorical", "has_capital_gain", "has_capital_loss")

y_train <- factor(train_data$target, levels=rev(levels(factor(test_data$target))))
x_train <- train_data[selected_attributes]
y_test <- factor(test_data$target, levels=rev(levels(factor(test_data$target))))
x_test <- test_data[selected_attributes]

# min-max normalización
normalize <- function(x_train, x_test) {
  return ((x_train - min(x_train, x_test)) / (max(x_train, x_test) - min(x_train, x_test)))
}

x_train$age <- normalize(x_train$age, x_test$age)
x_test$age <- normalize(x_test$age, x_train$age)

# one-hot
dmy <- dummyVars(" ~ .", data=x_train)
x_train <- data.frame(predict(dmy, newdata = x_train))

dmy <- dummyVars(" ~ .", data=x_test)
x_test <- data.frame(predict(dmy, newdata = x_test))

# creo una columna
x_test$native_country.Holand.Netherlands <- 0

# elimino las columnas que no se usan
x_train$sex.Male <- NULL
x_train$has_capital_gain.NO_capital_gain <- NULL
x_train$has_capital_loss.NO_capital_loss <- NULL

x_test$sex.Male <- NULL
x_test$has_capital_gain.NO_capital_gain <- NULL
x_test$has_capital_loss.NO_capital_loss <- NULL

# reordeno las columnas para que estén iguales que en train
x_test <- x_test[names(x_train)]
```

```{r}
# todas las combinaciones posibles
mlp_grid = expand.grid(layer1 = c(3, 5, 10),
                       layer2 = c(3, 5, 10),
                       layer3 = c(3, 5, 10))

# empleo 4 cores de mi procesador
cl <- makePSOCKcluster(4)
registerDoParallel(cl)

# se crean todos los modelos con los parámetros de mlp_grid (tuning) y se evalúa su rendimiento
model <- train(x_train, 
               y_train, 
               metric = "F",
               method = "mlpML", 
               tuneGrid = mlp_grid, 
               maxit = 100, 
               trControl = trainControl(method = "cv", summaryFunction = prSummary, verboseIter = TRUE, returnData = FALSE, number = 10, sampling = "down"), # number es el núm de veces que vuelve a entrenar
               preProcess = c("center", "scale", "nzv")) #nzv elimina las variables que no proporcionan conocimiento (varianza)

# dejo de emplear los 4 cores cuando he acabado de entrenar
stopCluster(cl)

#imprimo el modelo
model
```
```{r}
confusionMatrix(predict(model, newdata = x_train), y_train, mode = "prec_recall")
```


```{r}
confusionMatrix(predict(model, newdata = x_test), y_test, mode = "prec_recall")
```


###Instalacion de paquetes y librerias usadas
```{r}
install.packages(c('corrplot','GGally','cluster','factoextra','dendextend',"ClusterR"), repos = "http://cran.us.r-project.org")
```

```{r}
library(GGally)
library(corrplot)
library(cluster)
library(tidyverse)  
library(factoextra) 
library(dendextend)
library(ClusterR)
```
###Carga de datos y exploración de los mismos

```{r}

datos<-read.csv("dataset/heart.csv", sep = ",")

head(datos)
str(datos)
dim(datos) 
summary(datos)

datos_matrix <- as.matrix(datos[1:13])
#datos2 <- as.matrix(datos[1:5])
```


```{r}
# Escalado de datos y definición de la seed
set.seed(5)
datosNorm <- as.data.frame(scale(datos_matrix))
```
###CLustering Kmeans    
#Comprobación de k para cluster
```{r}
vector_compactacion<-0

for(i in 1:15){
  km_datos_aux2<-kmeans(datosNorm,center=i,nstar=20)
  vector_compactacion[i] <- km_datos_aux2$tot.withinss
}

par(mfrow = c(1,1))

# Representamos sum of squares vs. number of clusters
plot(1:15, vector_compactacion, type = "b", 
     xlab = "Numero de clusters", 
     #ylab = "Within groups sum of squares")
     ylab = "Compactacion")


```

```{r}
#Creación y representación de un cluster k2 para cada atributo de la tabla
datos_k2 <- kmeans(datosNorm, centers=2, nstar=25)

cluster <- datos_k2$cluster
datos_k2$size

ggpairs(cbind(datos, Cluster=as.factor(datos_k2$cluster)),
        columns=1:13, aes(colour=Cluster, alpha=0.5),
        lower=list(continuous="points"),
        upper=list(continuous="blank"),
        axisLabels="none", switch="both") +
        theme_bw()
```
###Validación externa kmeans

```{r}
table(datos$output, cluster)
```

```{r}
res = external_validation(datos$output, cluster, method = "adjusted_rand_index",summary_stats=TRUE)
```

###Clustering jerarquico
##Para realizar un cluster jerarquico aglomerativo, calculamos sus coeficientes para encontrar el método más eficaz
```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

coeficientes <- function(x) {
  agnes(datosNorm, method = x)$ac
}
map_dbl(m, coeficientes)

```

###Calculamos el dendograma mediante el método ward
```{r}

hclust.ward <- agnes(datosNorm, method = "ward")
pltree(hclust.ward, cex = 0.3, hang =-1, main = "Dendograma de agregación (metodo Ward)")
```
##Para calcular el número de clusters, podemos hacer aproximaciones similares a las realizadas en los cluster k-means
```{r}

#Método del codo
fviz_nbclust(datosNorm, FUN = hcut, method = "wss")
#Método de media de siluetas
fviz_nbclust(datosNorm, FUN = hcut, method = "silhouette")
#Método estadistica de brecha
brecha_estad <- clusGap(datosNorm, FUN = hcut, nstart = 25, K.max = 20, B = 50)
fviz_gap_stat(brecha_estad)
```

##Probamos con los valores k=2, k=3 y k= 5
```{r}

tree_aux2 <- cutree(as.hclust(hclust.ward), k = 2)
table(tree_aux2)
fviz_cluster(list(data = datosNorm, cluster = tree_aux2))

tree_aux <- cutree(as.hclust(hclust.ward), k = 3)
table(tree_aux)
fviz_cluster(list(data = datosNorm, cluster = tree_aux))

tree_aux <- cutree(as.hclust(hclust.ward), k = 5)
table(tree_aux)
fviz_cluster(list(data = datosNorm, cluster = tree_aux))

```

###Validación externa jerarquico aglomeración

```{r}
table(datos$output, tree_aux2)
```

```{r}
res = external_validation(datos$output, tree_aux2, method = "adjusted_rand_index",summary_stats=TRUE)
```
##Repetimos el experimento pero en esta ocasion para un cluster jerarquico de division
```{r}

hclust.diana <- diana(datosNorm)
tree_aux2 <- cutree(as.hclust(hclust.diana), k = 2)
table(tree_aux2)
fviz_cluster(list(data = datosNorm, cluster = tree_aux2))

tree_aux <- cutree(as.hclust(hclust.diana), k = 3)
table(tree_aux)
fviz_cluster(list(data = datosNorm, cluster = tree_aux))

tree_aux <- cutree(as.hclust(hclust.diana), k = 5)
table(tree_aux)
fviz_cluster(list(data = datosNorm, cluster = tree_aux))
```

###Validación externa jerarquico de división

```{r}
table(datos$output, tree_aux2)
```

```{r}
res = external_validation(datos$output, tree_aux2, method = "adjusted_rand_index",summary_stats=TRUE)
```

